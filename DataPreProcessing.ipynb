{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataPreProcessing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPGVFiR/a41Et45UT0O500V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weasel-codes/google-colab/blob/udemy-dl/DataPreProcessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIjv1FZ6Mfac"
      },
      "source": [
        "# **Impoting Libraries**\n",
        "We Will be importing 3 libraries\n",
        "* Numpy : Allows us to work with arrays\n",
        "* MathPlotLib : Plot some charts. Will mainly use pyplot\n",
        "* Panda : Import dataset and created matrices & vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDK568txMnew"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plot\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JFvUt_COXR5"
      },
      "source": [
        "# **Importing Datasets**\n",
        "* Will use pandas library to import dataset. Dataset is imported inside a variable.\n",
        "* Every ML problem has Features and dependent variables.\n",
        "  * Here Features are Country, Age, Salary\n",
        "  * Dependent VAriables : Purchased (usually last column of dataset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp4j6K8cOebo"
      },
      "source": [
        "dataset = pd.read_csv('Data.csv')\n",
        "# Create Entites using indexes of csv\n",
        "X = dataset.iloc[:, :-1].values #All the rows + all the columns except last\n",
        "Y = dataset.iloc[:,-1] #Dependent vairables is last column"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CFBNMsGT_fV"
      },
      "source": [
        "# **Taking Care of Missing Value**\n",
        "* We dont want missing data in ML dataset as it may cause error in training dataset.\n",
        "* Ignore data tupple if only 1% of such tupples are in dataset.\n",
        "* Replace missing data with Avg of all the values in which data is missing. This is classically performed\n",
        "\n",
        "Sykit learn is widely and very frequesntly used to work on datasets.\n",
        "* Import imputter class.\n",
        "* Create an object and first speciofy what type of values to replace with what and than use it on dataset.\n",
        "* Than use this object to fill missing values of dataset.\n",
        "  * fit() works only on numerical values and it is suggested to select all numerical columns at once since we are not sure which one of these columns might have missing values in huge datasets.\n",
        "  * Make sure to exclude string columns\n",
        "  * Call transform to it since we have linked funcationality.\n",
        "  * transform returns updated matrix so make sure you save it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBhVvqpXVynA"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imputer.fit(X[:,1:]) #connects to our matrix of features\n",
        "print(X)\n",
        "X[:,1:] = imputer.transform(X[:,1:]) #Sign to go forward\n",
        "print(\"Now updated >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> \")\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90XiY7HXdkOY"
      },
      "source": [
        "# **Encoding Categorical Data**\n",
        "* Trying to find relation btw features and outcome\n",
        "* We need to replace coountry column with three columns : one hawt coding\n",
        "* We will also do same for Purchased\n",
        "\n",
        "What we are trying to do is change our dataset in such a way that only numerical data remains.\n",
        "  * OneHotEncoding for several type of data\n",
        "  * LabelEncoding for 0s and 1s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXhIbns0fhf0"
      },
      "source": [
        "## Encoding Independent Variables\n",
        "* Will use two classes to do it \n",
        "  * CloumnTransformerClass : from Compose model of sykit library\n",
        "  * OneHotClass : from Preprocess model of same Sykit library\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R23bA7YVKav"
      },
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "# 'encoder' specifies that encoding ios needed to be done using OneHotEncoder\n",
        "# remainder specifies that only selected column needs to be encoded and remainder columns \n",
        "# needs to be added to appended to the result\n",
        "ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[0])],remainder='passthrough')\n",
        "\n",
        "# Now we need to attach this functionality to matrix\n",
        "X = np.array(ct.fit_transform(X)) # To create a matrix out of result\n",
        "\n",
        "X #100 : france, 010 : Germany, 001 : spain (alphabetical?)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHZleS-bIi9t"
      },
      "source": [
        "## Encoding Dependent Variables\n",
        "* We will use labelEncoder class to encode into 0 and 1 for resultant data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlWHVGITIzH_",
        "outputId": "110fa84b-83a0-4167-dd2d-54728c69ff25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "Y = le.fit_transform(Y)\n",
        "Y"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkXccdKhKMM6"
      },
      "source": [
        "# **Split Dataset in Training and Test Set**\n",
        "> Should we do feature scaling before or after division?\n",
        "\n",
        "After is the answer. We will be applying feature scaling on both type of data. \n",
        "\n",
        "Test Set is going to be like any real life new data coming in to get result. But if we are doing FS earlier, some of these type of data might of into Training and our system might learn it specifically. \n",
        "\n",
        "But when some data set comes that is totally unrelated to our test or training set, that might not give correct result... False positive case for have we trained properly?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAzuUS-SI4rv"
      },
      "source": [
        "# To create 4 separate set : 2 for training set of dependent and independent vars and same 2 for testing set\n",
        "# XTrain XTest YTrain YTest\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1) #80:20 :: Train:Test\n",
        "\n",
        "print(\"Train set for X : \\n\", X_train)\n",
        "print(\"\\n\\nTest set for X : \\n\", X_test)\n",
        "print(\"\\n\\nTrain set for Y : \\n\", Y_train)\n",
        "print(\"\\n\\nTest set for Y : \\n\", Y_test)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I--H_BUPh1wE"
      },
      "source": [
        "# **Feature Scaling**\n",
        "Why : Some of ML models we try to avoid some features dominating over others.\n",
        "\n",
        "Process : Standardisation and Normalisation\n",
        "\n",
        "Normalisation : when normal dist.\n",
        "\n",
        "Standardisation : works all the time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRFEuZmGhYQf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}